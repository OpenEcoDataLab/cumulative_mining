[
["index.html", "Cumulative mining impacts in Appalachian Rivers Introduction", " Cumulative mining impacts in Appalachian Rivers Dr. Matthew Ross matt.ross(at)colostate.edu August 31, 2020 Introduction For decades the rivers of Central Appalachia have been steadily buried (Ross, McGlynn, and Bernhardt 2016), polluted (Lindberg et al. 2011; Bernhardt and Palmer 2011; Bernhardt et al. 2012; Ross, McGlynn, and Bernhardt 2016; Ross et al. 2018; ???), and depopulated of their incredible biodiversity of macroinvertebrates (Voss and Bernhardt 2017) and fish (Hitt and Chambers 2014). The driver of all these impacts is mountaintop mining a process to extract shallow coal seams from beneath the surface of steep terrain. Mountaintop mining’s impacts are well-documented in the literature (Bernhardt and Palmer 2011), but most of these analyses linking mining to downstream impacts relied on relatively coarse temporal and spatial resolution maps of mining extent (Bernhardt et al. 2012) or datasets that were collected ad-hoc for each research paper (Lindberg et al. 2011). This problem of disaggregated and coarse mining coverage maps was fixed by Pericack and others who used Google Earth Engine to delineate active mining extent coverages for every year from 1985-2015 (Pericak et al. 2018). This new dataset is an excellent starting point to better understand the impacts of mountaintop mining on streams and rivers in Central Appalachia. However, these coverage maps are not linked to how water moves through these ecosystems, so there is still no publiclly available dataset showing how much mining is upstream of rivers in Central Appalachia. In this brief paper and video I produce annual maps of the total percent mining upstream for all of the watersheds in the Pericack dataset (???). I use open source terrain analysis tools like Whitebox (???), elevatr (https://cran.r-project.org/web/packages/elevatr/citation.html), and raster packages all in the R programming language (cite R). The code and data are all available on figshare and a permanent website version of the paper is here. "],
["methods.html", "1 Methods 1.1 Data acquisition and organization 1.2 Data preparation 1.3 Data Analysis", " 1 Methods To see how mining coverage has impacted rivers we need two datasets. First we need to know when mining first occured on the landscape. Second we need to know how these sections of mined lands are connected to river networks and how mining impacts might propagate downstream. The first dataset is part of the Pericack et al., paper (2018) labeled First Mining Year (GeoTIFF) in the figshare data repository. The second dataset we will create here using whitebox and elevation data from the region, but fist we need to download the data. 1.1 Data acquisition and organization 1.1.1 Study Area Let’s grab a shapefile of the study area from Pericack et al., 2018. This will be a zip of a shapefile, that we will need to unzip. area_file &lt;- &#39;data/in/study_area.zip&#39; if(!file.exists(area_file)){ download.file(&#39;https://ndownloader.figshare.com/articles/6253901?private_link=7a36745020ee5a517dcb&#39;, destfile=area_file,method=&#39;libcurl&#39;,mode=&#39;wb&#39;) unzip(area_file, exdir = &#39;data/in/study_area&#39;) } 1.1.2 Watershed boundaries Our study area is almost entirelyg within the 05 HUC 2 watershed basin from the USGS. We can download these shapefiles directly from the USGS here wbd_file &lt;- &#39;data/in/wbd_file.zip&#39; if(!file.exists(wbd_file)){ download.file(&#39;https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape/WBD_05_HU2_Shape.zip&#39;, destfile = wbd_file, method = &#39;libcurl&#39;, mode = &#39;wb&#39;) unzip(wbd_file, exdir = &#39;data/in/wbd&#39;) } 1.1.3 Flowlines We will likely want to use our data with the National Hydrography Dataset, so let’s download some flowlines using Dave Blodgett’s excellent nhdplusTools package. huc4_list &lt;- c(&#39;0510&#39;,&#39;0509&#39;,&#39;0507&#39;,&#39;0505&#39;,&#39;0513&#39;) if(!file.exists(&#39;data/in/nhd/simple_lines.gpkg&#39;)){ download_nhdplushr(&#39;data/in/nhd&#39;,huc4_list, download_files = T) d &lt;- get_nhdplushr(&#39;data/in/nhd/05&#39;, layers = &#39;NHDFlowline&#39;)$NHDFlowline #Get rid of small accumulated ares to make downstream calcs faster d1 &lt;- d %&gt;% filter(TotDASqKM &gt;= 1) %&gt;% st_transform(2163) %&gt;% st_simplify(.,dTolerance = 500) #Subset only large rivers for visualizing and testing burnout approach d3 &lt;- d %&gt;% dplyr::filter(TotDASqKM &gt;= 200) %&gt;% st_transform(2163) %&gt;% st_simplify(.,dTolerance = 500) st_write(d3,&#39;data/in/nhd/simple_lines.gpkg&#39;,delete_dsn=T) st_write(d1,&#39;data/in/nhd/lines_1km.gpkg&#39;,delete_dsn=T) } 1.1.4 Cumulative mining from Pericack et al., 2018 Here we are just downloading the cumulative mining data from the Pericack study #Look for cumulative mining data cume_file &lt;- &#39;data/in/cume.tif&#39; #Check if the file already exists, if not run the commands below. cume_downloaded = !file.exists(cume_file) if(cume_downloaded){ #Create a data repo if one doesn&#39;t exist. dir.create(&#39;data&#39;) dir.create(&#39;data/in&#39;) dir.create(&#39;data/out&#39;) #Download data locally. Link is from paper download.file(&#39;https://ndownloader.figshare.com/files/11446991?private_link=e99954fc2876c6e96a7c&#39;,destfile=cume_file,method=&#39;libcurl&#39;,mode=&#39;wb&#39;) } p### Download relevant NHD raster data For this analysis we will want to do some of our own terrain analysis work, but first we need to download the National Elevation Data snapshots from the NHDPlus website, since we will be using their flow accumulation data as well. All of this data can be found here, and our regions are 05a,c,and d. B is outside of our study region. dem_download_table &lt;- tibble(type = c( &#39;fac&#39;,&#39;fac&#39;,&#39;fac&#39;, &#39;dem&#39;,&#39;dem&#39;,&#39;dem&#39;), name = rep(c(&#39;a&#39;,&#39;c&#39;,&#39;d&#39;), times = 2), file = c(&#39;https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05a_FdrFac_01.7z&#39;,&#39;https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05c_FdrFac_01.7z&#39;,&#39;https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05d_FdrFac_01.7z&#39;,&#39;https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05a_NEDSnapshot_01.7z&#39;,&#39;https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05c_NEDSnapshot_01.7z&#39;,&#39;https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05d_NEDSnapshot_01.7z&#39;), destfile = paste0(&#39;data/in/nhd_dem/&#39;,type,&#39;_&#39;,name,&#39;.7z&#39;), destfolder = gsub(&#39;\\\\.7z&#39;,&#39;&#39;,destfile)) dir.create(&#39;data/in/nhd_dem&#39;) if(!file.exists(dem_download_table$destfile[1])){ nhd_dem_downloader &lt;- function(type,name,file, destfile, destfolder){ download.file(file, destfile = destfile, mode = &#39;wb&#39;) #7zip extraction e = extracat -0 = output folder #y = yes to all 7z questions #r = recursive extraction #-spf = put files back in folders they are zipped in system(paste0(&#39;7z e &#39;,destfile,&#39; -o&#39;,destfolder,&#39; -y -r -spf&#39;)) #This works but produces a deeply nested output file. Easily addressed, but #annoying. file.remove(destfile) } #Walk over this data and collect outputs! pwalk(dem_download_table,nhd_dem_downloader) } 1.1.5 Merge DEMs from above Still data acquisition? Questionable, but just merging with very fast terra. You’ll need lots of memory to do this (I needed 10gb ram) if(!file.exists(&#39;data/in/nhd_dem.tif&#39;)){ #These tables are basically instruction tables merge_table &lt;- dem_download_table %&gt;% mutate(destfolder = gsub(&#39;\\\\.7z&#39;,&#39;&#39;,destfile), full_paths = ifelse(type == &#39;fac&#39;, paste0(destfolder, &#39;/NHDPlusMS/NHDPlus05/NHDPlusFdrFac05&#39;, name,&#39;/fac/w001001.adf&#39;), paste0(destfolder, &#39;/NHDPlusMS/NHDPlus05/NEDSnapshot/Ned05&#39;, name,&#39;/elev_cm/w001001.adf&#39;))) %&gt;% dplyr::select(type,full_paths) %&gt;% group_by(type) %&gt;% nest() %&gt;% rename(files = data) crs_rast &lt;- merge_table %&gt;% unnest(files) %&gt;% pull(full_paths) %&gt;% .[1]%&gt;% raster(.) area &lt;- st_read(&#39;data/in/study_area/Study-Area.shp&#39;) %&gt;% st_transform(.,st_crs(crs_rast)) terra_merge &lt;- function(type,files){ files = unnest(files, cols = c()) %&gt;% pull(full_paths) container &lt;- list() for(i in 1:length(files)){ out &lt;- raster(files[i]) %&gt;% crop(.,area) if(grepl(&#39;elev_cm&#39;,files[i])){ #convert to meters out &lt;- out/100 } container[[i]] &lt;- out } #Sadly terra had unexpected behavior here so terra::merge is really raster merge merged &lt;- do.call(merge,container) writeRaster(merged,filename = paste0(&#39;data/in/nhd_&#39;,type,&#39;.tif&#39;), overwrite = T) } pwalk(merge_table,terra_merge) } 1.1.5.1 Using elevatr This is not recommended because for such large scale analysis, we should use the DEMs provided directly from the USGS instead of the aggregated and tiled DEMs from elevatr. I left this here because elevatr is great and would likely still work with some tweaks. raw_dem_file &lt;- &#39;data/in/elev_raw.tif&#39; raw_dem_eval &lt;- !file.exists(raw_dem_file) if(raw_dem_eval){ #Download data from elevatr (mapzen) cume_r &lt;- raster(&#39;data/in/cume.tif&#39;) elev_raw &lt;- get_elev_raster(cume_r,z=11) #Save raw file writeRaster(elev_raw,raw_dem_file,overwrite=T) } 1.1.5.2 Reproject elevation data into same projection as cumulative mining tif. The elevatr comes down in a different projection than the cumulative mining dataset, so we need to reproject it so that the cells match exactly in resolution and location proj_dem_file &lt;- &#39;data/out/elev.tif&#39; reproject &lt;- !file.exists(proj_dem_file) if(reproject){ elev_raw &lt;- rast(&#39;data/in/elev_raw.tif&#39;) cume &lt;- rast(&#39;data/in/cume.tif&#39;) #Project raster into cumulative raster cells and projection elev &lt;- project(elev_raw,cume) #Save this elev data for whitebox terra::writeRaster(elev,proj_dem_file,overwrite=T) } 1.2 Data preparation To prepare our data for whitebox terrain analyses, we need to process our cumulative mining layer so that we produce 31 rasters that have cumulative mining coverage up to that year. So we will make a tif called “1984.tif” and it will only contain mining up to 1984, and then 1985, ’86 and so on. To demonstrate this process we’ll look at a county where there has been a lot of mining so we can see how cumulative mining changes over time, Boone County, WV. 1.2.1 Picking “whole” flowlines for analysis A critical aspect of figuring out the percent of mining in river networks is understanding if our mining coverage dataset from [Pericack et al., 2015] covers the entire watershed. Otherwise we could have a river (like the Ohio), which has large portions that it drains from outside of the coverage of our data. Here we will use the amazing NHDPlusTools package from Dave Blodgett to find all the flowlines that originate outside of our study area. Later we will “burn” these lines out of our analysis. if(!file.exists(&#39;data/out/nhd/burns.gpkg&#39;)){ ## Read in spatially simplified flowlines simple_lines &lt;- st_read(&#39;data/in/nhd/simple_lines.gpkg&#39;) ## Read in almost complete flowline data km1_lines &lt;- st_read(&#39;data/in/nhd/lines_1km.gpkg&#39;) #Get our study area and reproject area &lt;- st_read(&#39;data/in/study_area/Study-Area.shp&#39;) %&gt;% st_transform(., st_crs(km1_lines)) #This creates a thin 2000m band around the study area (and outside of the study) #Area by 50m. This skinny band will then be used to grab intersecting #flowlines which will tell us rivers that cross out of our study area boundary area_zone &lt;- area %&gt;% #Bring the edge out 2000m st_buffer(2050) %&gt;% #Cast the polygon as a line st_cast(.,&#39;LINESTRING&#39;) %&gt;% #Buffer the line by 2000m st_buffer(2000) #Find lines that are outside our study area out_lines &lt;- km1_lines %&gt;% group_by(LevelPathI) %&gt;% filter(Pathlength == min(Pathlength)) %&gt;% ungroup() %&gt;% filter(st_intersects(., area_zone, sparse = F)) #Get COMIDs for the mainstems that fall outside our study area mainstems_out &lt;- out_lines$COMID #Trace areas downstream of these comids downstream_of_out &lt;- function(comid,network = km1_lines){ burn_out &lt;- get_DM(comid,network = network) } #Run in parallel (slight speed improvements) plan(multiprocess) burn_outs &lt;- future_map(mainstems_out,downstream_of_out) #Unlist the map output burn_out_vector &lt;- unlist(burn_outs) %&gt;% unique(.) within_lines &lt;- km1_lines %&gt;% .[area,] burns &lt;- within_lines %&gt;% filter(COMID %in% burn_out_vector) within_simple &lt;- simple_lines %&gt;% .[area,] burn_simple &lt;- within_simple %&gt;% filter(COMID %in% burn_out_vector) #Careful these are simplified lines. #For burning them into our analysis we will want to use #the full lines from get_nhdplus. write.csv(burn_out_vector, &#39;data/out/burn_comids.csv&#39;) st_write(within_lines, &#39;data/out/nhd/within_lines.gpkg&#39;) st_write(burns, &#39;data/out/nhd/burns.gpkg&#39;) st_write(within_simple, &#39;data/out/nhd/within_simple.gpkg&#39;) st_write(burn_simple, &#39;data/out/nhd/burn_simple.gpkg&#39;) } charleston &lt;- us_cities() %&gt;% filter(city == &#39;Charleston&#39; &amp; state_abbr == &#39;WV&#39;) %&gt;% st_transform(2163) %&gt;% st_buffer(50000) ## Visualize simplified lines around charleston within_simple &lt;- st_read(&#39;data/out/nhd/within_simple.gpkg&#39;) %&gt;% .[charleston,] burn_simple &lt;- st_read(&#39;data/out/nhd/burn_simple.gpkg&#39;) %&gt;% .[charleston,] #Get our study area and reproject c_area &lt;- st_read(&#39;data/in/study_area/Study-Area.shp&#39;) %&gt;% st_transform(., st_crs(within_simple)) mapview(within_simple, color = &#39;blue&#39;, layer.name = &#39;Contained river&#39;) + mapview(burn_simple, color = &#39;red&#39;, layer.name = &#39;Burn out river&#39;) + mapview(c_area, col.region = &#39;gray30&#39;, layer.name = &#39;Study area&#39;, homebutton = F) 1.2.2 Picking whole huc12s In addition to working only with only rivers that are entirely inside our study area, we will also work with huc12s that are within our area only. This will get rid of some areas on the southern extent of our mining detectino area that are in the HUC4 region. There is relatively little mining in this area, but this code could be used to produce the huc 4 percent mining if so desired. huc_12 &lt;- st_read(&#39;data/in/wbd/Shape/WBDHU12.shp&#39;) %&gt;% st_transform(2163) huc4 &lt;- st_read(&#39;data/in/wbd/Shape/WBDHU4.shp&#39;) study_12s &lt;- huc_12 %&gt;% filter(st_within(., c_area %&gt;% #In case hucs are on border st_buffer(50), sparse = F)) %&gt;% ## In the southwestern and ester most corner of the region, there are three huc12s # That are discontinuous with the rest of the study region. We will explicitly #remove those here. filter(!huc12 %in% c(&#39;051301080301&#39;, &#39;051301080703&#39;, &#39;051301080904&#39;, &#39;050500020401&#39;)) #Simplify for display visual_12s &lt;- study_12s %&gt;% st_simplify(.,dTolerance = 2000) mapview(c_area, layer.name = &#39;Original Study Area&#39;) + mapview(visual_12s, col.regions = &#39;red&#39;,layer.name = &#39;Whole HUC 12s&#39;) #Save the new study area if(!file.exists(&#39;data/out/whole_area.gpkg&#39;)){ new_area &lt;- study_12s %&gt;% summarize() %&gt;% #Buffer this area by 30m to make sure we capture top of ridgeline #from hydrodems st_buffer(30) st_write(new_area,&#39;data/out/whole_area.gpkg&#39;,delete_layer = T) } 1.2.3 Trim all analysis rasters to new study area extent We have a series of raster’s that we downloaded and merged upstream (flow accumulation elevation, cumulative mining). These all need to be trimmed to our new study area. We will use #Read in as a stars object, because terra doesn&#39;t play nice with sf yet for #reprojecting. if(!file.exists(&#39;data/out/elev_study.tif&#39;)){ #Transform everything to the elev.tif projection (this takes a while) elev &lt;- rast(&#39;data/in/nhd_dem.tif&#39;) cume &lt;- rast(&#39;data/in/cume.tif&#39;) %&gt;% project(.,elev, method = &#39;near&#39;, mask = T) #Fix an issue from the reprojection (some cells = -Inf) cume[cume&lt; 1984] &lt;- NA #Change all within bound NAs to 0s (mass flux fix?) cume[is.na(cume)] &lt;- 0 fac &lt;- rast(&#39;data/in/nhd_fac.tif&#39;) #Trim to new huc5 study area new_area &lt;- vect(&#39;data/out/whole_area.gpkg&#39;) %&gt;% project(.,crs(elev)) #Crop and mask those rasters elev_study &lt;- crop(elev, new_area) %&gt;% mask(.,new_area) cume_study &lt;- crop(cume, new_area) %&gt;% mask(.,new_area) fac_study &lt;- crop(fac, new_area) %&gt;% mask(., new_area) #Save the data writeRaster(elev_study, &#39;data/out/elev_study.tif&#39;, overwrite = T) writeRaster(cume_study, &#39;data/out/cume_study.tif&#39;,overwrite = T) writeRaster(fac_study, &#39;data/out/fac_study.tif&#39;, overwrite = T) } 1.2.4 First Mining Year for Boone County elev &lt;- rast(&#39;data/out/elev_study.tif&#39;) cume &lt;- rast(&#39;data/out/cume_study.tif&#39;) boone &lt;- us_counties(states=&#39;West Virginia&#39;) %&gt;% filter(name == &#39;Boone&#39;) %&gt;% #Match projections to mining data st_transform(crs(cume)) #Crop to boone cume_boone &lt;- crop(cume,boone) cume_boone[cume_boone == 0] &lt;- NA elev_boone &lt;- crop(elev,boone) #Terra doesn&#39;t work with tmap (yet presumably) #So we have to use base! #OMG HCL.colors is nice plot(cume_boone,col=hcl.colors(30,&#39;viridis&#39;),frame=F,axes=F) plot(elev_boone,col=hcl.colors(20,&#39;Grays&#39;),add=T,frame=F,axes=F, legend = F) plot(cume_boone,col=hcl.colors(30,&#39;viridis&#39;),add=T, legend = F) 1.2.5 Cumulative mining Boone County 1990 Just an example of how we are creating these rasters and what they will look like. #Set all values above a year value to NA and all values before or equal to 1990 to 1 cut_year &lt;- 2015 rcl &lt;- matrix(c(cut_year + 1,2016,NA, 1984,cut_year,cut_year),nrow=2,ncol=3,byrow=T) cume_cut &lt;- classify(cume_boone,rcl,lowest = T) plot(elev_boone,col=hcl.colors(20,&#39;Grays&#39;),frame=F,axes=F) plot(cume_cut,col=&#39;red&#39;,add=T,useRaster=T, legend = F) 1.2.6 Animated loop showing how this looks for all years cut_years = 1984:2015 for(i in cut_years){ rcl &lt;- matrix(c(i+1,2016,NA, 1984,i,1),nrow=2,ncol=3,byrow=T) cume_cuts &lt;- terra::classify(cume_boone,rcl, lowest = T) terra::plot(elev_boone,col=hcl.colors(20,&#39;Grays&#39;), add=F,frame=F,axes=F,main=paste(&#39;Cumulative Mining Growth&#39;,i)) terra::plot(cume_cuts,col=&#39;red&#39;,add=T,leg.shrink=.3, legend = F) } 1.2.7 Filtering and outputting cumulative mining rasters 1984-2015 if(!dir.exists(&#39;data/out/annual_cumes&#39;)){ dir.create(&#39;data/out/annual_cumes&#39;) } #Years for cume data years &lt;- 1984:2015 year_files &lt;- paste0(&#39;data/out/annual_cumes/&#39;,years,&#39;.tif&#39;) #Rerun? cumer_run &lt;- !all(file.exists(year_files)) # Making a function for reclassifying Matrix for every year. mine_cumer &lt;- function(year){ #Reclassify matrix rcl &lt;- matrix(c(year+1,Inf,0, 1984,year+1,1),nrow=2,ncol=3,byrow=T) #Reclassify raster cume_cut &lt;- classify(cume,rcl, lowest = T) #write it out file = paste0(&#39;data/out/annual_cumes/&#39;,year,&#39;.tif&#39;) terra::writeRaster(cume_cut,filename=file,overwrite=T) } #THis is why all the TERRA stuff is worth it about 100X faster than #raster and 20X faster than using futures and furrr mapping if(cumer_run){ map(2015,mine_cumer) } 1.2.8 Final whitebox preparation The primary whitebox function we will be using to generate our cumulative mining maps will be D8MassFlux. This tool takes four different rasters A DEM (we will use NHD hydrologically conditioned DEM) A loading raster (which will be our annual cumulative mining rasters) An efficiency raster which we will set to 1 (% mining is not mitigated as you go downstream) An absorption raster which we will set to zero (nothing interrupts the flow of mining down the network) We will make the efficiency and absorption raster’s by reading in our cumulative mining dataset and setting all values to zero. area &lt;- vect(&#39;data/out/whole_area.gpkg&#39;) zero &lt;- rast(&#39;data/out/cume_study.tif&#39;) %&gt;% setValues(0) %&gt;% mask(.,area) one &lt;- rast(&#39;data/out/cume_study.tif&#39;) %&gt;% setValues(1) %&gt;% mask(.,area) writeRaster(zero,&#39;data/out/zero.tif&#39;) writeRaster(one,&#39;data/out/one.tif&#39;) 1.3 Data Analysis 1.3.1 Breach DEMs got issues sometimes, if(!dir.exists(&#39;data/out/wbt_outputs/annual_accumed&#39;)){ dir.create(&#39;data/out/wbt_outputs/annual_accumed&#39;, recursive = T) } writeRaster(elev_boone,&#39;data/out/elev_boone.tif&#39;) #Dist needs tuning. Looked wonky until set at 20000 wbt_breach_depressions_least_cost(&#39;data/out/elev_study.tif&#39;, &#39;data/out/wbt_outputs/breached.tif&#39;, dist = 20000, min_dist = T, fill = T) 1.3.2 Targetted flow accumulation loading &lt;- list.files(&#39;data/out/annual_cumes&#39;,full.names = T) dem &lt;- &#39;data/out/wbt_outputs/breached.tif&#39; absorption &lt;- &#39;data/out/zero.tif&#39; efficiency &lt;- &#39;data/out/one.tif&#39; accumulated &lt;- gsub(&#39;annual_cumes&#39;,&#39;wbt_outputs/annual_accumed&#39;, loading) #Loop over each year if(!all(file.exists(accumulated))){ for(i in 1:length(years)){ wbt_d8_mass_flux(dem = dem, loading = loading[i], efficiency = efficiency, absorption = absorption, output = accumulated[i], verbose_mode = T) } } boone_check &lt;- raster(&#39;data/out/wbt_outputs/annual_accumed/2015.tif&#39;) boone_l &lt;- log10(boone_check) "]
]
