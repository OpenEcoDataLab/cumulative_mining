

# Methods


To see how mining coverage has impacted rivers we need two datasets. First
we need to know when mining first occured on the landscape. Second we need
to know how these sections of mined lands are connected to river networks and
how mining impacts might propagate downstream. The first dataset is part of the
Pericack et al., paper (2018) labeled `First Mining Year (GeoTIFF)` in the 
figshare data repository. The second dataset we will create here using `whitebox` and elevation data from the region, but fist we need to download the data. 

```{r setup, include=FALSE}
library(sf) #Amazingly simple tidy GIS R package
library(mapview) #Interactive mapping of sf objects
library(tidyverse) #Good 'ol tidyverse (dplyr, readr, more)
library(elevatr) #R access to mapzen (dynamic downloading of DEMs)
library(raster) # Name says it all, rasters in R
library(whitebox) #amazing terrain analysis package
library(stars) # (spatiotemporal objects)
library(USAboundaries)
library(tmap) 
library(furrr) #Parallel mapping
library(animation)
library(terra) # much faster raster
library(nhdplusTools) #Navigate america's rivers
knitr::opts_chunk$set(echo = TRUE,eval = TRUE,
                      cache = TRUE, warning = FALSE,
                      message = FALSE)

par(mar = c(0,0,0,2))
# Run once!
#devtools::install_github("giswqs/whiteboxR")
```




## Data acquisition and organization


### Study Area

Let's grab a shapefile of the study area from Pericack et al., 2018. This will
be a zip of a shapefile, that we will need to unzip. 

```{r}

area_file <- 'data/in/study_area.zip'

if(!file.exists(area_file)){
download.file('https://ndownloader.figshare.com/articles/6253901?private_link=7a36745020ee5a517dcb',
              destfile=area_file,method='libcurl',mode='wb')

unzip(area_file, exdir = 'data/in/study_area')
}


```

### Watershed boundaries


Our study area is almost entirelyg within the 05 HUC 2 watershed basin from the USGS.
We can download these shapefiles directly from the USGS [here](https://catalog.data.gov/dataset/usgs-national-watershed-boundary-dataset-wbd-downloadable-data-collection-national-geospatial-) 


```{r}


wbd_file <- 'data/in/wbd_file.zip'

if(!file.exists(wbd_file)){
download.file('https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape/WBD_05_HU2_Shape.zip',
              destfile = wbd_file, method = 'libcurl', mode = 'wb')

unzip(wbd_file, exdir = 'data/in/wbd')

}
```
  



### Flowlines

We will likely want to use our data with the National Hydrography Dataset,
so let's download some flowlines using Dave Blodgett's excellent `nhdplusTools`
package.

```{r}


huc4_list <- c('0510','0509','0507','0505','0513')



if(!file.exists('data/in/nhd/simple_lines.gpkg')){
  download_nhdplushr('data/in/nhd',huc4_list, download_files = T)
  
  d <- get_nhdplushr('data/in/nhd/05',
                     layers = 'NHDFlowline')$NHDFlowline
  
  #Get rid of small accumulated ares to make downstream calcs faster
  d1 <- d %>%
    filter(TotDASqKM >= 1) %>%
    st_transform(2163) %>%
    st_simplify(.,dTolerance = 500) 

  #Subset only large rivers for visualizing and testing burnout approach
  d3 <- d %>%
    dplyr::filter(TotDASqKM >= 200) %>%
    st_transform(2163) %>%
    st_simplify(.,dTolerance = 500) 

  
  st_write(d3,'data/in/nhd/simple_lines.gpkg',delete_dsn=T)
  st_write(d1,'data/in/nhd/lines_1km.gpkg',delete_dsn=T)
}



```




### Cumulative mining from Pericack et al., 2018

Here we are just downloading the cumulative mining data from the Pericack study

```{r download cume}


#Look for cumulative mining data
cume_file <- 'data/in/cume.tif'

#Check if the file already exists, if not run the commands below. 
cume_downloaded = !file.exists(cume_file)


if(cume_downloaded){
  #Create a data repo if one doesn't exist.
  dir.create('data')
  dir.create('data/in')
  dir.create('data/out')
  
  #Download data locally. Link is from paper
  download.file('https://ndownloader.figshare.com/files/11446991?private_link=e99954fc2876c6e96a7c',destfile=cume_file,method='libcurl',mode='wb')
  

}


```

p### Download relevant NHD raster data

For this analysis we will want to do some of our own terrain analysis work, but
first we need to download the National Elevation Data snapshots from the NHDPlus
website, since we will be using their flow accumulation data as well.
All of this data can be found [here](https://www.epa.gov/waterdata/nhdplus-ohio-data-vector-processing-unit-05),
and our regions are 05a,c,and d. B is outside of our study region. 

```{r}


dem_download_table <- tibble(type = c(
                                      'fac','fac','fac',
                                      'dem','dem','dem'),
                             name = rep(c('a','c','d'), times = 2),
                             file = c('https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05a_FdrFac_01.7z','https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05c_FdrFac_01.7z','https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05d_FdrFac_01.7z','https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05a_NEDSnapshot_01.7z','https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05c_NEDSnapshot_01.7z','https://s3.amazonaws.com/edap-nhdplus/NHDPlusV21/Data/NHDPlusMS/NHDPlus05/NHDPlusV21_MS_05_05d_NEDSnapshot_01.7z'),
                             destfile = paste0('data/in/nhd_dem/',type,'_',name,'.7z'),
                             destfolder = gsub('\\.7z','',destfile))
  

dir.create('data/in/nhd_dem')

if(!file.exists(dem_download_table$destfile[1])){
nhd_dem_downloader <- function(type,name,file, destfile, destfolder){
  download.file(file,
                destfile = destfile,
                mode = 'wb')

  #7zip extraction e = extracat -0 = output folder
  #y = yes to all 7z questions
  #r = recursive extraction
  #-spf = put files back in folders they are zipped in
  system(paste0('7z e ',destfile,' -o',destfolder,' -y -r -spf'))
  #This works but produces a deeply nested output file. Easily addressed, but
  #annoying.
  file.remove(destfile)
}

#Walk over this data and collect outputs!

  pwalk(dem_download_table,nhd_dem_downloader)
}


  

```


### Merge DEMs from above

Still data acquisition? Questionable, but just merging with very fast terra.
You'll need lots of memory to do this (I needed 10gb ram)
```{r}


if(!file.exists('data/in/nhd_dem.tif')){
  #These tables are basically instruction tables
  merge_table <- dem_download_table %>%
    mutate(destfolder = gsub('\\.7z','',destfile),
           full_paths = ifelse(type == 'fac',
                               paste0(destfolder,
                                      '/NHDPlusMS/NHDPlus05/NHDPlusFdrFac05',
                                      name,'/fac/w001001.adf'),
                              paste0(destfolder,
                                     '/NHDPlusMS/NHDPlus05/NEDSnapshot/Ned05',
                                     name,'/elev_cm/w001001.adf'))) %>%
    dplyr::select(type,full_paths) %>%
    group_by(type) %>%
    nest() %>%
    rename(files = data)
  
  crs_rast <- merge_table %>%
    unnest(files) %>%
    pull(full_paths) %>%
    .[1]%>%
    raster(.)
  
  area <- st_read('data/in/study_area/Study-Area.shp') %>%
    st_transform(.,st_crs(crs_rast))
  
  
  terra_merge <- function(type,files){
    files = unnest(files, cols = c()) %>%
      pull(full_paths)
    container <- list()
    for(i in 1:length(files)){
      out <- raster(files[i]) %>%
        crop(.,area) 
      
      if(grepl('elev_cm',files[i])){
        #convert to meters
        out <- out/100 
      }
      
      container[[i]] <- out
        
    }
    #Sadly terra had unexpected behavior here so terra::merge is really raster merge
    merged <- do.call(merge,container)
    
    writeRaster(merged,filename =
                         paste0('data/in/nhd_',type,'.tif'),
                       overwrite = T)
  }
  
  
  pwalk(merge_table,terra_merge)
}


```


#### Using elevatr

This is not recommended because for such large scale analysis, we should
use the DEMs provided directly from the USGS instead of the aggregated and 
tiled DEMs from `elevatr`. I left this here because `elevatr` is great and would
likely still work with some tweaks. 


```{r elev download, eval = F}


raw_dem_file <- 'data/in/elev_raw.tif'
raw_dem_eval <- !file.exists(raw_dem_file)


if(raw_dem_eval){
  #Download data from elevatr (mapzen)
  cume_r <- raster('data/in/cume.tif')
  elev_raw <- get_elev_raster(cume_r,z=11)
  #Save raw file
  writeRaster(elev_raw,raw_dem_file,overwrite=T)
}
```


#### Reproject elevation data into same projection as cumulative mining tif. 

The elevatr comes down in a different projection than the cumulative mining 
dataset, so we need to reproject it so that the cells match exactly in
resolution and location


```{r reproject, eval = F}
proj_dem_file <- 'data/out/elev.tif'
reproject <- !file.exists(proj_dem_file)

if(reproject){
  elev_raw <- rast('data/in/elev_raw.tif')
  cume <- rast('data/in/cume.tif')
  #Project raster into cumulative raster cells and projection
  elev <- project(elev_raw,cume)
  #Save this elev data for whitebox
  terra::writeRaster(elev,proj_dem_file,overwrite=T)
}
```


## Data preparation

To prepare our data for whitebox terrain analyses, we need to process our
cumulative mining layer so that we produce 31 rasters that have cumulative mining
coverage up to that year. So we will make a tif called "1984.tif" and it will 
only contain mining up to 1984, and then 1985, '86 and so on. To demonstrate
this process we'll look at a county where there has been a lot of mining so we 
can see how cumulative mining changes over time, Boone County, WV.

### Picking "whole" flowlines for analysis

A critical aspect of figuring out the percent of mining in river networks is
understanding if our mining coverage dataset from [Pericack et al., 2015] covers
the entire watershed. Otherwise we could have a river (like the Ohio), which has
large portions that it drains from outside of the coverage of our data. Here we
will use the amazing `NHDPlusTools` package from Dave Blodgett to find all
the flowlines that originate outside of our study area. Later we will "burn"
these lines out of our analysis. 


```{r}

if(!file.exists('data/out/nhd/burns.gpkg')){
  ## Read in spatially simplified flowlines
  simple_lines <- st_read('data/in/nhd/simple_lines.gpkg') 
  
  ## Read in almost complete flowline data
  km1_lines <- st_read('data/in/nhd/lines_1km.gpkg')
  
  
  #Get our study area and reproject
  area <- st_read('data/in/study_area/Study-Area.shp') %>%
    st_transform(., st_crs(km1_lines)) 
  
  #This creates a thin 2000m band around the study area (and outside of the study)
  #Area by 50m. This skinny band will then be used to grab intersecting 
  #flowlines which will tell us rivers that cross out of our study area boundary
  area_zone <- area %>%
    #Bring the edge out 2000m
    st_buffer(2050) %>%
    #Cast the polygon as a line
    st_cast(.,'LINESTRING') %>%
    #Buffer the line by 2000m
    st_buffer(2000)
  
  
  #Find lines that are outside our study area
  out_lines <- km1_lines %>%
    group_by(LevelPathI) %>%
    filter(Pathlength == min(Pathlength)) %>%
    ungroup() %>%
    filter(st_intersects(., area_zone, sparse = F))
  
  
  #Get COMIDs for the mainstems that fall outside our study area
  mainstems_out <- out_lines$COMID
    
  
  
  #Trace areas downstream of these comids
  downstream_of_out <- function(comid,network = km1_lines){
    burn_out <- get_DM(comid,network = network)
  }
  
  #Run in parallel (slight speed improvements)
  plan(multiprocess)
  burn_outs <- future_map(mainstems_out,downstream_of_out)
  
  #Unlist the map output
  burn_out_vector <- unlist(burn_outs) %>% unique(.)
  
  
  within_lines <- km1_lines %>%
    .[area,]
  
  burns <- within_lines %>%
    filter(COMID %in% burn_out_vector)
  
  within_simple <- simple_lines %>%
    .[area,]
  
  
  burn_simple <- within_simple %>%
    filter(COMID %in% burn_out_vector)
  
  #Careful these are simplified lines. 
  #For burning them into our analysis we will want to use
  #the full lines from get_nhdplus. 
  write.csv(burn_out_vector, 'data/out/burn_comids.csv')
  st_write(within_lines, 'data/out/nhd/within_lines.gpkg')
  st_write(burns, 'data/out/nhd/burns.gpkg') 
  st_write(within_simple, 'data/out/nhd/within_simple.gpkg')
  st_write(burn_simple, 'data/out/nhd/burn_simple.gpkg')
}

charleston <- us_cities() %>%
  filter(city == 'Charleston' & state_abbr == 'WV') %>%
  st_transform(2163) %>%
  st_buffer(50000)


## Visualize simplified lines around charleston
within_simple <- st_read('data/out/nhd/within_simple.gpkg') %>%
  .[charleston,]

burn_simple <- st_read('data/out/nhd/burn_simple.gpkg') %>%
  .[charleston,]

  #Get our study area and reproject
c_area <- st_read('data/in/study_area/Study-Area.shp') %>%
  st_transform(., st_crs(within_simple))  

mapview(within_simple, color = 'blue', layer.name = 'Contained river') + 
  mapview(burn_simple, color = 'red', layer.name = 'Burn out river') + 
  mapview(c_area, col.region = 'gray30', layer.name = 'Study area', homebutton = F)
```

### Picking whole huc12s

In addition to working only with only rivers that are entirely inside our
study area, we will also work with huc12s that are within our area only. This
will get rid of some areas on the southern extent of our mining detectino area
that are in the HUC4 region. There is relatively little mining in this area, but
this code could be used to produce the huc 4 percent mining if so desired. 


```{r}

huc_12 <- st_read('data/in/wbd/Shape/WBDHU12.shp') %>% 
  st_transform(2163)


huc4 <- st_read('data/in/wbd/Shape/WBDHU4.shp')


study_12s <- huc_12 %>%
  filter(st_within(.,
                   c_area  %>% #In case hucs are on border
                     st_buffer(50),
                   sparse = F)) %>%
  ## In the southwestern and ester most corner of the region, there are three huc12s
# That are discontinuous with the rest of the study region. We will explicitly 
#remove those here. 
  filter(!huc12 %in% c('051301080301',
                       '051301080703',
                       '051301080904',
                       '050500020401'))






#Simplify for display
visual_12s <- study_12s %>%
  st_simplify(.,dTolerance = 2000)

mapview(c_area, layer.name = 'Original Study Area') + 
mapview(visual_12s, col.regions = 'red',layer.name = 'Whole HUC 12s') 

#Save the new study area
if(!file.exists('data/out/whole_area.gpkg')){
  new_area <- study_12s %>%
    summarize() %>%
    #Buffer this area by 30m to make sure we capture top of ridgeline
    #from hydrodems
    st_buffer(30)
  st_write(new_area,'data/out/whole_area.gpkg',delete_layer = T)
}


```

### Trim all analysis rasters to new study area extent

We have a series of raster's that we downloaded and merged upstream (flow accumulation
elevation, cumulative mining). These all need to be trimmed to our new study area.
We will use 

```{r}
#Read in as a stars object, because terra doesn't play nice with sf yet for 
#reprojecting. 


if(!file.exists('data/out/elev_study.tif')){
  #Transform everything to the elev.tif projection (this takes a while)
  elev <- rast('data/in/nhd_dem.tif') 
  
  cume <- rast('data/in/cume.tif') %>%
    project(.,elev, method = 'near', mask = T)
  
  #Fix an issue from the reprojection (some cells = -Inf)
  cume[cume< 1984] <- NA
  
  #Change all within bound NAs to 0s (mass flux fix?)
  cume[is.na(cume)] <- 0
  
  fac <- rast('data/in/nhd_fac.tif')
  
  #Trim to new huc5 study area
  new_area <- vect('data/out/whole_area.gpkg') %>%
    project(.,crs(elev))
  
  #Crop and mask those rasters
  elev_study <- crop(elev, new_area) %>%
    mask(.,new_area)
  
  cume_study <- crop(cume, new_area) %>%
    mask(.,new_area)
  
  fac_study <- crop(fac, new_area) %>%
    mask(., new_area)
  
  #Save the data
  writeRaster(elev_study, 'data/out/elev_study.tif', overwrite = T)
  writeRaster(cume_study, 'data/out/cume_study.tif',overwrite = T)
  writeRaster(fac_study, 'data/out/fac_study.tif', overwrite = T)
}

```



### First Mining Year for Boone County


```{r first,fig.width= 7,fig.height= 7,fig.cap = 'First year of mountaintop mining in Boone County West Virginia', cache = F}

elev <- rast('data/out/elev_study.tif')

cume <- rast('data/out/cume_study.tif')


boone <- us_counties(states='West Virginia') %>%
  filter(name == 'Boone') %>%
  #Match projections to mining data
  st_transform(crs(cume))

#Crop to boone
cume_boone <- crop(cume,boone) 
cume_boone[cume_boone == 0] <- NA

elev_boone <- crop(elev,boone)


#Terra doesn't work with tmap (yet presumably)
#So we have to use base!



#OMG HCL.colors is nice
plot(cume_boone,col=hcl.colors(30,'viridis'),frame=F,axes=F)
plot(elev_boone,col=hcl.colors(20,'Grays'),add=T,frame=F,axes=F, legend = F)
plot(cume_boone,col=hcl.colors(30,'viridis'),add=T, legend = F)





```

### Cumulative mining Boone County 1990

Just an example of how we are creating these rasters and what they will look 
like. 

```{r cume,fig.width = 7,fig.height = 7,fig.caption='Cumulative mining as of 1990'}
#Set all values above a year value to NA and all values before or equal to 1990 to 1

cut_year <- 2015
rcl <- matrix(c(cut_year + 1,2016,NA,
              1984,cut_year,cut_year),nrow=2,ncol=3,byrow=T)


cume_cut <- classify(cume_boone,rcl,lowest = T)

plot(elev_boone,col=hcl.colors(20,'Grays'),frame=F,axes=F)
plot(cume_cut,col='red',add=T,useRaster=T, legend = F)


```

### Animated loop showing how this looks for all years


```{r boone gif, fig.height = 7,fig.width = 7,  fig.caption = 'Cumulative annual mining and last year mined',interval=0.2, animation.hook='gifski'}


cut_years = 1984:2015




for(i in cut_years){
  
  rcl <- matrix(c(i+1,2016,NA,
            1984,i,1),nrow=2,ncol=3,byrow=T)
  
  cume_cuts <- terra::classify(cume_boone,rcl, lowest = T)
  
  
  
terra::plot(elev_boone,col=hcl.colors(20,'Grays'),
       add=F,frame=F,axes=F,main=paste('Cumulative Mining Growth',i))
  terra::plot(cume_cuts,col='red',add=T,leg.shrink=.3,
              legend = F)

}


```



### Filtering and outputting cumulative mining rasters 1984-2015

```{r}

if(!dir.exists('data/out/annual_cumes')){
  dir.create('data/out/annual_cumes')
}

#Years for cume data
years <- 1984:2015

year_files <- paste0('data/out/annual_cumes/',years,'.tif')

#Rerun? 
cumer_run <- !all(file.exists(year_files))


# Making a function for reclassifying Matrix for every year. 

mine_cumer <- function(year){
  #Reclassify matrix
  rcl <- matrix(c(year+1,Inf,0,
              1984,year+1,1),nrow=2,ncol=3,byrow=T)
  
  
  #Reclassify raster
  cume_cut <- classify(cume,rcl, lowest = T)
  
  #write it out
  file = paste0('data/out/annual_cumes/',year,'.tif')
  terra::writeRaster(cume_cut,filename=file,overwrite=T)
}

#THis is why all the TERRA stuff is worth it about 100X faster than
#raster and 20X faster than using futures and furrr mapping
if(cumer_run){
map(2015,mine_cumer)
}
```




### Final whitebox preparation 

The primary `whitebox` function we will be using to generate our cumulative 
mining maps will be `D8MassFlux`. This tool takes four different rasters

- A DEM (we will use NHD hydrologically conditioned DEM)

- A loading raster (which will be our annual cumulative mining rasters)

- An efficiency raster which we will set to 1 (% mining is not mitigated as you go downstream)

- An absorption raster which we will set to zero (nothing interrupts the flow of mining down the network)

We will make the efficiency and absorption raster's by reading in our cumulative
mining dataset and setting all values to zero.

```{r}

area <- vect('data/out/whole_area.gpkg')

zero <- rast('data/out/cume_study.tif') %>%
  setValues(0) %>%
  mask(.,area)



one <- rast('data/out/cume_study.tif') %>%
  setValues(1) %>%
  mask(.,area)

writeRaster(zero,'data/out/zero.tif')
writeRaster(one,'data/out/one.tif')

```



## Data Analysis

### Breach

DEMs got issues sometimes, 

```{r}

if(!dir.exists('data/out/wbt_outputs/annual_accumed')){
  dir.create('data/out/wbt_outputs/annual_accumed', recursive = T)
}

writeRaster(elev_boone,'data/out/elev_boone.tif')

#Dist needs tuning. Looked wonky until set at 20000
wbt_breach_depressions_least_cost('data/out/elev_study.tif',
                             'data/out/wbt_outputs/breached.tif',
                             dist = 20000,
                             min_dist = T,
                             fill = T)


```


### Targetted flow accumulation
```{r}


loading <- list.files('data/out/annual_cumes',full.names = T)
dem <- 'data/out/wbt_outputs/breached.tif'
absorption <- 'data/out/zero.tif'
efficiency <- 'data/out/one.tif'



accumulated  <- gsub('annual_cumes','wbt_outputs/annual_accumed', loading)

#Loop over each year
if(!all(file.exists(accumulated))){
  for(i in 1:length(years)){
    wbt_d8_mass_flux(dem = dem,
                     loading = loading[i],
                     efficiency = efficiency,
                     absorption = absorption,
                     output = accumulated[i],
                     verbose_mode = T)
  }
}
```




```{r}
boone_check <- raster('data/out/wbt_outputs/annual_accumed/2015.tif') 

boone_l <- log10(boone_check)


```

